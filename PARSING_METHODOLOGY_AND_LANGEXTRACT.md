# ğŸ§  Parsing Methodology & LangExtract Integration

**Date:** October 17, 2025  
**Project:** OP Choudhary Social Media Analytics Dashboard

---

## ğŸ“š **TABLE OF CONTENTS**

1. [Current Parsing Methodology](#current-parsing-methodology)
2. [How Our Parser Works](#how-our-parser-works)
3. [LangExtract Integration Possibilities](#langextract-integration-possibilities)
4. [Human-in-the-Loop Learning System](#human-in-the-loop-learning-system)
5. [Implementation Roadmap](#implementation-roadmap)

---

## 1. CURRENT PARSING METHODOLOGY

### **Overview:**
Our parsing system is a **hybrid approach** that combines:
- **Regex-based pattern matching** (for Hindi entities)
- **AI-powered semantic understanding** (for context and classification)
- **Human validation** (for training data quality)

### **Input:**
```
"à¤…à¤‚à¤¤à¤¾à¤—à¤¢à¤¼ à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾ à¤•à¥‡ à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤µà¤¿à¤§à¤¾à¤¯à¤• à¤à¤µà¤‚ à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼ à¤­à¤¾à¤œà¤ªà¤¾ à¤•à¥‡ à¤ªà¥‚à¤°à¥à¤µ à¤…à¤§à¥à¤¯à¤•à¥à¤· 
à¤®à¤¾à¤¨à¤¨à¥€à¤¯ à¤¶à¥à¤°à¥€ à¤µà¤¿à¤•à¥à¤°à¤® à¤‰à¤¸à¥‡à¤‚à¤¡à¥€ à¤œà¥€ à¤•à¥‹ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤•à¥€ à¤¹à¤¾à¤°à¥à¤¦à¤¿à¤• à¤¬à¤§à¤¾à¤ˆ à¤à¤µà¤‚ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤¯à¥‡à¤‚à¥¤"
```

### **Output:**
```json
{
  "event_type": "birthday_wishes",
  "event_type_confidence": 0.95,
  "locations": [
    {"name": "à¤…à¤‚à¤¤à¤¾à¤—à¤¢à¤¼", "type": "constituency"},
    {"name": "à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼", "type": "state"}
  ],
  "people_mentioned": ["à¤µà¤¿à¤•à¥à¤°à¤® à¤‰à¤¸à¥‡à¤‚à¤¡à¥€"],
  "organizations": ["à¤­à¤¾à¤œà¤ªà¤¾"],
  "schemes_mentioned": [],
  "overall_confidence": 0.78
}
```

---

## 2. HOW OUR PARSER WORKS

### **Step-by-Step Process:**

#### **STEP 1: Text Preprocessing**
```python
# File: api/src/parsing/enhanced_parser.py

def preprocess(text: str) -> str:
    """
    Clean and normalize Hindi text
    """
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Normalize Unicode (handle different Hindi encodings)
    text = unicodedata.normalize('NFC', text)
    
    # Remove URLs, mentions, hashtags (if needed)
    # text = re.sub(r'https?://\S+', '', text)
    
    return text
```

**What happens:**
- Removes extra spaces
- Normalizes Unicode characters (important for Hindi!)
- Optionally removes URLs, @mentions

---

#### **STEP 2: Entity Extraction (Regex-based)**

Our enhanced parser uses **comprehensive regex patterns** for Hindi entities:

```python
# Define patterns for common places
PLACE_KEYWORDS = re.compile(
    r'(à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€|à¤¨à¤¯à¥€ à¤¦à¤¿à¤²à¥à¤²à¥€|à¤°à¤¾à¤¯à¤—à¤¢à¤¼|à¤¦à¤¿à¤²à¥à¤²à¥€|à¤°à¤¾à¤¯à¤ªà¥à¤°|à¤­à¤¾à¤°à¤¤|à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼|'
    r'à¤–à¤°à¤¸à¤¿à¤¯à¤¾|à¤—à¤¢à¤¼ à¤‰à¤®à¤°à¤¿à¤¯à¤¾|à¤¬à¤¸à¥à¤¤à¤°|à¤¸à¤°à¤—à¥à¤œà¤¾|à¤œà¤¶à¤ªà¥à¤°|à¤¬à¤—à¥€à¤šà¤¾|à¤…à¤‚à¤¤à¤¾à¤—à¤¢à¤¼)',
    re.IGNORECASE
)

# Pattern for "X à¤®à¥‡à¤‚" (in X) - context-based location detection
LOCATION_CONTEXT_PATTERN = re.compile(
    r'([\u0900-\u097F\u0600-\u06FF\u0020-\u007E\s]{2,}?)\s+à¤®à¥‡à¤‚',
    re.UNICODE
)

def _extract_locations(self, text: str) -> List[Dict[str, str]]:
    """
    Extract locations using multiple strategies
    """
    locations = set()
    
    # Strategy 1: Direct keyword matching
    for match in PLACE_KEYWORDS.finditer(text):
        locations.add(match.group(0))
    
    # Strategy 2: Context-based ("X à¤®à¥‡à¤‚" pattern)
    for match in LOCATION_CONTEXT_PATTERN.finditer(text):
        potential_loc = match.group(1).strip()
        # Verify it's a known location
        if PLACE_KEYWORDS.search(potential_loc):
            locations.add(potential_loc)
    
    # Convert to structured format
    return [
        {"name": loc, "type": self._classify_location_type(loc)} 
        for loc in locations
    ]
```

**What happens:**
1. **Direct Matching**: Looks for known place names (à¤°à¤¾à¤¯à¤—à¤¢à¤¼, à¤¦à¤¿à¤²à¥à¤²à¥€, etc.)
2. **Context Matching**: Finds patterns like "à¤…à¤‚à¤¤à¤¾à¤—à¤¢à¤¼ à¤®à¥‡à¤‚" (in Antagarh)
3. **Validation**: Cross-checks against known place database

---

#### **STEP 3: People Extraction**

```python
PEOPLE_KEYWORDS = re.compile(
    r'(à¤µà¤¿à¤•à¥à¤°à¤® à¤‰à¤¸à¥‡à¤‚à¤¡à¥€|à¤°à¤®à¤¨ à¤¸à¤¿à¤‚à¤¹|à¤­à¥‚à¤ªà¥‡à¤¶ à¤¬à¤˜à¥‡à¤²|à¤…à¤œà¥€à¤¤ à¤œà¥‹à¤—à¥€|à¤¦à¤¿à¤²à¥€à¤ª à¤¸à¤¿à¤‚à¤¹ à¤œà¥‚à¤¦à¥‡à¤µ|'
    r'à¤“à¤® à¤ªà¥à¤°à¤•à¤¾à¤¶ à¤šà¥Œà¤§à¤°à¥€|à¤“à¤ªà¥€ à¤šà¥Œà¤§à¤°à¥€|à¤ªà¤µà¤¨ à¤ªà¤Ÿà¥‡à¤²|à¤­à¤°à¤¤ à¤²à¤¾à¤² à¤ªà¤Ÿà¥‡à¤²)',
    re.IGNORECASE
)

# Pattern for "à¤¶à¥à¤°à¥€ X à¤œà¥€" (honorific patterns)
HONORIFIC_PATTERN = re.compile(
    r'(?:à¤®à¤¾à¤¨à¤¨à¥€à¤¯\s+)?(?:à¤¶à¥à¤°à¥€|à¤¶à¥à¤°à¥€à¤®à¤¤à¥€|à¤¡à¥‰\.?|à¤ªà¥à¤°à¥‹\.?)\s+([\u0900-\u097F\s]+?)\s*à¤œà¥€',
    re.UNICODE
)

def _extract_people(self, text: str) -> List[str]:
    """
    Extract people names using patterns
    """
    people = set()
    
    # Strategy 1: Known names
    for match in PEOPLE_KEYWORDS.finditer(text):
        people.add(match.group(0))
    
    # Strategy 2: Honorific patterns
    for match in HONORIFIC_PATTERN.finditer(text):
        name = match.group(1).strip()
        if len(name.split()) <= 4:  # Reasonable name length
            people.add(name)
    
    return list(people)
```

**What happens:**
1. **Known Names**: Matches against politician database
2. **Pattern Detection**: Finds "à¤¶à¥à¤°à¥€ X à¤œà¥€" patterns
3. **Validation**: Filters out false positives (too long/short names)

---

#### **STEP 4: Event Type Classification**

This is where **AI/semantic understanding** helps:

```python
def _classify_event_type(self, text: str) -> Tuple[str, float]:
    """
    Classify the type of event/activity
    """
    text_lower = text.lower()
    
    # Rule-based classification with confidence scores
    event_patterns = {
        'birthday_wishes': (
            ['à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨', 'à¤œà¤¨à¥à¤®à¤¦à¤¿à¤µà¤¸', 'à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤¯à¥‡à¤‚', 'à¤¬à¤§à¤¾à¤ˆ'],
            0.95
        ),
        'meeting': (
            ['à¤¬à¥ˆà¤ à¤•', 'à¤®à¥€à¤Ÿà¤¿à¤‚à¤—', 'à¤šà¤°à¥à¤šà¤¾', 'à¤µà¤¿à¤šà¤¾à¤°-à¤µà¤¿à¤®à¤°à¥à¤¶'],
            0.90
        ),
        'rally': (
            ['à¤°à¥ˆà¤²à¥€', 'à¤œà¤¨à¤¸à¤­à¤¾', 'à¤¸à¤­à¤¾', 'à¤®à¤¹à¤¾à¤¸à¤­à¤¾'],
            0.90
        ),
        'inspection': (
            ['à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£', 'à¤¦à¥Œà¤°à¤¾', 'à¤¸à¤®à¥€à¤•à¥à¤·à¤¾'],
            0.85
        ),
        'inauguration': (
            ['à¤‰à¤¦à¥à¤˜à¤¾à¤Ÿà¤¨', 'à¤²à¥‹à¤•à¤¾à¤°à¥à¤ªà¤£', 'à¤¶à¤¿à¤²à¤¾à¤¨à¥à¤¯à¤¾à¤¸', 'à¤­à¥‚à¤®à¤¿à¤ªà¥‚à¤œà¤¨'],
            0.90
        ),
        'scheme_announcement': (
            ['à¤¯à¥‹à¤œà¤¨à¤¾', 'à¤˜à¥‹à¤·à¤£à¤¾', 'à¤¶à¥à¤°à¥à¤†à¤¤', 'à¤²à¤¾à¤‚à¤š'],
            0.85
        ),
        'condolence': (
            ['à¤¶à¥‹à¤•', 'à¤¦à¥à¤–', 'à¤¨à¤¿à¤§à¤¨', 'à¤¸à¥à¤µà¤°à¥à¤—à¤µà¤¾à¤¸', 'à¤¶à¥à¤°à¤¦à¥à¤§à¤¾à¤‚à¤œà¤²à¤¿'],
            0.90
        ),
    }
    
    best_match = ('other', 0.30)  # Default
    
    for event_type, (keywords, base_confidence) in event_patterns.items():
        matches = sum(1 for keyword in keywords if keyword in text_lower)
        if matches > 0:
            # Confidence increases with multiple keyword matches
            confidence = min(base_confidence + (matches - 1) * 0.05, 0.99)
            if confidence > best_match[1]:
                best_match = (event_type, confidence)
    
    return best_match
```

**What happens:**
1. **Pattern Matching**: Looks for keywords associated with event types
2. **Confidence Scoring**: More matching keywords = higher confidence
3. **Best Match**: Returns highest-confidence classification

---

#### **STEP 5: Confidence Calculation**

```python
def _calculate_overall_confidence(
    self,
    event_confidence: float,
    has_locations: bool,
    has_people: bool,
    has_organizations: bool,
    has_schemes: bool
) -> float:
    """
    Calculate overall parsing confidence
    """
    # Base confidence from event classification
    confidence = event_confidence * 0.4
    
    # Add confidence for extracted entities
    if has_locations:
        confidence += 0.25
    if has_people:
        confidence += 0.20
    if has_organizations:
        confidence += 0.10
    if has_schemes:
        confidence += 0.05
    
    return round(confidence, 2)
```

**What happens:**
- **Event classification** contributes 40%
- **Locations** contribute 25%
- **People** contribute 20%
- **Organizations** contribute 10%
- **Schemes** contribute 5%

---

### **Complete Flow Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Tweet Text â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing  â”‚ â† Clean, normalize Unicode
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Entity Extractionâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Locations     â”‚ â† Regex patterns + context
â”‚ â€¢ People        â”‚ â† Honorific patterns + DB
â”‚ â€¢ Organizations â”‚ â† Known org patterns
â”‚ â€¢ Schemes       â”‚ â† Scheme keywords
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Event Classificationâ”‚ â† Keyword matching + scoring
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Confidence Calc  â”‚ â† Weighted entity presence
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Structured JSON â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. LANGEXTRACT INTEGRATION POSSIBILITIES

### **What is LangExtract?**

LangExtract is a powerful tool for **multilingual entity extraction** that:
- Uses **linguistic patterns** to identify entities
- Provides **entity codes** (like ISO codes for languages)
- Builds **internal knowledge graphs** connecting similar entities
- Works across **multiple languages** (perfect for Hindi/English mix!)

### **Current Example (Your Understanding):**

```
Tweet: "à¤°à¤¾à¤¯à¤—à¤¢à¤¼ à¤®à¥‡à¤‚ à¤¬à¥ˆà¤ à¤•"

LangExtract Output:
{
  "entities": [
    {
      "text": "à¤°à¤¾à¤¯à¤—à¤¢à¤¼",
      "type": "LOCATION",
      "code": "LOC_IN_CG_RAI",  â† Standardized code
      "confidence": 0.92,
      "aliases": ["Raigarh", "à¤°à¤¾à¤¯à¤—à¤¢", "RAIGARH"]
    },
    {
      "text": "à¤¬à¥ˆà¤ à¤•",
      "type": "EVENT",
      "code": "EVENT_MEETING",
      "confidence": 0.88,
      "related_terms": ["à¤®à¥€à¤Ÿà¤¿à¤‚à¤—", "meeting", "à¤šà¤°à¥à¤šà¤¾"]
    }
  ]
}
```

---

### **How LangExtract Would Enhance Our System:**

#### **Benefit 1: Entity Normalization**

**Problem:** Different spellings of same entity
```
à¤°à¤¾à¤¯à¤—à¤¢à¤¼ = à¤°à¤¾à¤¯à¤—à¤¢ = Raigarh = RAIGARH
```

**LangExtract Solution:**
```python
{
  "canonical_name": "à¤°à¤¾à¤¯à¤—à¤¢à¤¼",
  "code": "LOC_IN_CG_RAI",
  "variants": ["à¤°à¤¾à¤¯à¤—à¤¢", "Raigarh", "RAIGARH", "RÃ¡yagaá¹›h"]
}
```

All variants map to **same code** â†’ Better analytics!

---

#### **Benefit 2: Knowledge Graph Building**

**Problem:** Can't find relationships between entities

**LangExtract Solution:**
```
à¤°à¤¾à¤¯à¤—à¤¢à¤¼ (LOC_IN_CG_RAI)
  â”œâ”€â”€ is_part_of â†’ à¤›à¤¤à¥à¤¤à¥€à¤¸à¤—à¤¢à¤¼ (LOC_IN_CG)
  â”œâ”€â”€ is_part_of â†’ à¤­à¤¾à¤°à¤¤ (LOC_IN)
  â”œâ”€â”€ has_constituency â†’ à¤°à¤¾à¤¯à¤—à¤¢à¤¼ à¤µà¤¿à¤§à¤¾à¤¨à¤¸à¤­à¤¾ (CONST_CG_RAI)
  â””â”€â”€ associated_people â†’ à¤“à¤ªà¥€ à¤šà¥Œà¤§à¤°à¥€ (PERSON_OPCH)
```

This lets us answer questions like:
- "How many tweets mention Chhattisgarh constituencies?"
- "What events happen most in Raigarh district?"

---

#### **Benefit 3: Cross-Language Matching**

**Problem:** Mixed Hindi/English tweets

```
Tweet 1: "à¤°à¤¾à¤¯à¤—à¤¢à¤¼ à¤®à¥‡à¤‚ rally"
Tweet 2: "Raigarh à¤®à¥‡à¤‚ à¤œà¤¨à¤¸à¤­à¤¾"
```

**LangExtract Solution:**
```
rally    â†’ CODE: EVENT_RALLY
à¤œà¤¨à¤¸à¤­à¤¾   â†’ CODE: EVENT_RALLY

Both map to same code! âœ…
```

---

### **Proposed Integration Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          TWEET INGESTION                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CURRENT PARSER (Regex + AI)                 â”‚
â”‚  â€¢ Fast first-pass extraction                   â”‚
â”‚  â€¢ Identifies potential entities                â”‚
â”‚  â€¢ Assigns preliminary confidence               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HUMAN REVIEW QUEUE                      â”‚
â”‚  â€¢ Low confidence items (<70%)                  â”‚
â”‚  â€¢ Ambiguous entities                           â”‚
â”‚  â€¢ New entity types                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ (User Approves)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LANGEXTRACT PROCESSING                  â”‚
â”‚  â€¢ Entity normalization                         â”‚
â”‚  â€¢ Code assignment                              â”‚
â”‚  â€¢ Knowledge graph update                       â”‚
â”‚  â€¢ Cross-reference matching                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CANONICAL ENTITY STORE                      â”‚
â”‚  {                                              â”‚
â”‚    "à¤°à¤¾à¤¯à¤—à¤¢à¤¼": {                                  â”‚
â”‚      "code": "LOC_IN_CG_RAI",                   â”‚
â”‚      "variants": [...],                         â”‚
â”‚      "relationships": [...],                    â”‚
â”‚      "tweet_count": 45,                         â”‚
â”‚      "first_seen": "2025-01-01",                â”‚
â”‚      "confidence": 0.95                         â”‚
â”‚    }                                            â”‚
â”‚  }                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ANALYTICS & INSIGHTS                    â”‚
â”‚  â€¢ Normalized entity counts                    â”‚
â”‚  â€¢ Relationship-based queries                  â”‚
â”‚  â€¢ Multi-language aggregation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. HUMAN-IN-THE-LOOP LEARNING SYSTEM

### **Current State:**
- Human reviews parsed tweets
- Can edit entities inline
- Corrections stored locally

### **Proposed Enhancement:**

#### **Phase 1: Correction Storage**
```typescript
// When human corrects a tweet
{
  "tweet_id": "1978808458720797118",
  "corrections": {
    "event_type": {
      "before": "rally",
      "after": "birthday_wishes",
      "confidence_before": 0.45,
      "confidence_after": 1.0
    },
    "locations": {
      "added": ["à¤…à¤‚à¤¤à¤¾à¤—à¤¢à¤¼"],
      "removed": [],
      "reason": "Missed constituency name"
    }
  },
  "reviewed_by": "human",
  "reviewed_at": "2025-10-17T14:30:00Z"
}
```

#### **Phase 2: Pattern Learning**

When human makes corrections, system learns:

```python
# Example: Human repeatedly corrects "à¤°à¥ˆà¤²à¥€" â†’ birthday_wishes

# System learns new pattern:
NEW_PATTERN = {
  "trigger": "à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨",
  "event_type": "birthday_wishes",
  "confidence": 0.95,
  "learned_from": ["tweet_123", "tweet_456"],
  "approval_count": 5
}

# Add to event classification rules
self.learned_patterns.append(NEW_PATTERN)
```

#### **Phase 3: Feedback Loop**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Tweet    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parse with   â”‚
â”‚ Current Rulesâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     YES      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Confidence   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Auto-approve â”‚
â”‚ >= 80%?      â”‚               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
       â”‚ NO                            â”‚
       â–¼                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚ Human Review â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
       â”‚                               â”‚
       â–¼                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚ Corrections? â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
       â”‚ YES                           â”‚
       â–¼                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚ Learn Patternâ”‚                      â”‚
â”‚ Update Rules â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
       â”‚                               â”‚
       â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Store in Database              â”‚
â”‚   (with learned patterns)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Phase 4: Pattern Confidence**

```python
class PatternLearner:
    def __init__(self):
        self.patterns = {}
    
    def learn_from_correction(self, correction):
        """
        Learn new patterns from human corrections
        """
        # Extract pattern
        trigger = correction['trigger_words']
        result = correction['correct_classification']
        
        # Create pattern signature
        pattern_id = f"{trigger}_{result}"
        
        if pattern_id not in self.patterns:
            self.patterns[pattern_id] = {
                'trigger': trigger,
                'result': result,
                'confidence': 0.6,  # Start low
                'approval_count': 1,
                'rejection_count': 0,
                'examples': [correction['tweet_id']]
            }
        else:
            # Increase confidence with more approvals
            pattern = self.patterns[pattern_id]
            pattern['approval_count'] += 1
            pattern['examples'].append(correction['tweet_id'])
            
            # Confidence formula
            total = pattern['approval_count'] + pattern['rejection_count']
            pattern['confidence'] = pattern['approval_count'] / total
    
    def get_active_patterns(self, min_confidence=0.8, min_examples=3):
        """
        Get patterns ready for production use
        """
        return [
            p for p in self.patterns.values()
            if p['confidence'] >= min_confidence 
            and len(p['examples']) >= min_examples
        ]
```

---

## 5. IMPLEMENTATION ROADMAP

### **Phase 1: Enhanced Human Review UI** (Now!)
- âœ… Editable fields for all entities
- âœ… Add/remove entities dynamically
- âœ… Confidence override
- âœ… Correction reasoning field
- âœ… Keyboard shortcuts (A=Approve, E=Edit, R=Reject)

### **Phase 2: Correction Storage** (Next)
- Store corrections in `parsed_events` table
- Add `corrections_log` column (JSONB)
- Track correction history
- API endpoint for submitting corrections

### **Phase 3: Pattern Learning** (Week 2)
- Build `PatternLearner` class
- Extract patterns from corrections
- Test patterns against validation set
- Promote high-confidence patterns to production

### **Phase 4: LangExtract Integration** (Week 3-4)
- Set up LangExtract service
- Build entity code mapping
- Create knowledge graph database
- Implement cross-language matching

### **Phase 5: Advanced Analytics** (Week 5+)
- Relationship-based queries
- Multi-language aggregation
- Predictive event classification
- Automated tagging suggestions

---

## ğŸ“Š **EXPECTED IMPROVEMENTS:**

| Metric | Current | After Phase 3 | After Phase 4 |
|--------|---------|---------------|---------------|
| Location Detection | 77% | 85% | 95% |
| People Extraction | 66% | 78% | 90% |
| Event Classification | 85% | 90% | 95% |
| Cross-language Matching | N/A | N/A | 95% |
| Entity Normalization | 60% | 70% | 98% |

---

## ğŸ¯ **KEY TAKEAWAYS:**

1. **Current System**: Hybrid regex + AI approach
2. **Strengths**: Fast, reasonable accuracy, language-specific
3. **Weaknesses**: No normalization, no cross-language, limited learning
4. **LangExtract Benefit**: Entity codes, knowledge graphs, multi-language
5. **Human Learning**: Corrections â†’ Patterns â†’ Better parsing
6. **Final Vision**: Self-improving, multi-language, world-class parser

---

**This document will be updated as we implement each phase.**

