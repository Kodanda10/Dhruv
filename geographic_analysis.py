#!/usr/bin/env python3
"""
Geographic Mapping & Mindmap Analysis for OP Choudhary tweets
Analyzes location mentions and creates hierarchical geographic insights
"""

import re
import json
from collections import Counter, defaultdict
from datetime import datetime
import statistics

def decode_unicode_escapes(text):
    """Convert RTF Unicode escapes to actual characters"""
    def replace_unicode(match):
        code = int(match.group(1))
        try:
            return chr(code)
        except ValueError:
            return match.group(0)  # Keep original if invalid

    # Handle \uXXXX patterns with spaces (RTF format)
    text = re.sub(r'\\u(\d{4,5})\s*', replace_unicode, text)

    # Handle consecutive Unicode escapes
    text = re.sub(r'\\u(\d{4,5})\\u(\d{4,5})', lambda m: chr(int(m.group(1))) + chr(int(m.group(2))), text)

    # Clean up RTF control codes
    text = re.sub(r'\\[a-z]+\d*', '', text)
    text = re.sub(r'\\uc\d+', '', text)

    return text

def parse_rtf_tweets(rtf_content):
    """Parse tweets from RTF content"""
    tweets = []

    # Split by tweet sections
    tweet_sections = re.split(r'Tweet #\d+', rtf_content)[1:]  # Skip header

    for i, section in enumerate(tweet_sections, 1):
        tweet = {}

        # Extract ID
        id_match = re.search(r'ID: (\d+)', section)
        tweet['id'] = id_match.group(1) if id_match else f'unknown_{i}'

        # Extract date
        date_match = re.search(r'Date: ([^\n]+)', section)
        tweet['date'] = date_match.group(1).strip() if date_match else 'unknown'

        # Extract author
        author_match = re.search(r'Author: (@[^\n]+)', section)
        tweet['author'] = author_match.group(1).strip() if author_match else 'unknown'

        # Extract status
        status_match = re.search(r'Status: ([^\n]+)', section)
        tweet['status'] = status_match.group(1).strip() if status_match else 'unknown'

        # Extract text (between "Text:" and "Metrics:")
        text_match = re.search(r'Text:([\s\S]*?)(?=Metrics:)', section)
        if text_match:
            text = text_match.group(1).strip()
            # Remove RTF braces and control codes first
            text = re.sub(r'[{}]', '', text)
            text = decode_unicode_escapes(text)
            # Clean up extra whitespace and line breaks
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
            tweet['text'] = text
        else:
            tweet['text'] = ''

        # Extract metrics
        likes_match = re.search(r'Likes: (\d+)', section)
        tweet['likes'] = int(likes_match.group(1)) if likes_match else 0

        replies_match = re.search(r'Replies: (\d+)', section)
        tweet['replies'] = int(replies_match.group(1)) if replies_match else 0

        retweets_match = re.search(r'Retweets: (\d+)', section)
        tweet['retweets'] = int(retweets_match.group(1)) if retweets_match else 0

        quotes_match = re.search(r'Quotes: (\d+)', section)
        tweet['quotes'] = int(quotes_match.group(1)) if quotes_match else 0

        tweet['engagement'] = tweet['likes'] + tweet['replies'] + tweet['retweets'] + tweet['quotes']

        tweets.append(tweet)

    return tweets

def extract_locations(text):
    """Extract location mentions from tweet text"""
    locations = []

    # Chhattisgarh districts and major cities
    districts = [
        '‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º', '‡§ï‡•ã‡§∞‡§¨‡§æ', '‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞', '‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞', '‡§¶‡•Å‡§∞‡•ç‡§ó', '‡§∞‡§æ‡§ú‡§®‡§æ‡§Ç‡§¶‡§ó‡§æ‡§Ç‡§µ',
        '‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞-‡§ö‡§æ‡§Ç‡§™‡§æ', '‡§ï‡§¨‡•Ä‡§∞‡§ß‡§æ‡§Æ', '‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º', '‡§Æ‡§π‡§æ‡§∏‡§Æ‡•Å‡§Ç‡§¶', '‡§ß‡§Æ‡§§‡§∞‡•Ä',
        '‡§â‡§§‡•ç‡§§‡§∞ ‡§¨‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ‡§Ç‡§ï‡•á‡§∞', '‡§¨‡§∏‡•ç‡§§‡§∞', '‡§ï‡•ã‡§Ç‡§°‡§æ‡§ó‡§æ‡§Ç‡§µ', '‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞', '‡§¶‡§Ç‡§§‡•á‡§µ‡§æ‡§°‡§º‡§æ',
        '‡§¨‡•Ä‡§ú‡§æ‡§™‡•Å‡§∞', '‡§∏‡•Å‡§ï‡§Æ‡§æ', '‡§ó‡§∞‡§ø‡§Ø‡§æ‡§¨‡§Ç‡§¶', '‡§¨‡§≤‡•ã‡§¶', '‡§¨‡§≤‡•å‡§¶‡§æ ‡§¨‡§æ‡§ú‡§æ‡§∞', '‡§ó‡•å‡§∞‡•á‡§≤‡§æ-‡§™‡•á‡§Ç‡§°‡•ç‡§∞‡§æ-‡§Æ‡§∞‡§µ‡§æ‡§π‡•Ä'
    ]

    # Major towns and blocks
    towns_blocks = [
        '‡§ñ‡§∞‡§∏‡§ø‡§Ø‡§æ', '‡§ß‡§∞‡§Æ‡§ú‡§Ø‡§ó‡§¢‡§º', '‡§ó‡•å‡§∞‡•á‡§≤‡§æ', '‡§Æ‡§®‡•á‡§Ç‡§¶‡•ç‡§∞‡§ó‡§¢‡§º', '‡§Ö‡§Ç‡§§‡§æ‡§ó‡§¢‡§º', '‡§™‡§Ç‡§°‡§∞‡§ø‡§Ø‡§æ',
        '‡§≤‡•à‡§≤‡•Ç‡§Ç‡§ó‡§æ', '‡§¨‡§∞‡§Æ‡§ï‡§≤‡§æ', '‡§ï‡•ã‡§∞‡§¨‡§æ', '‡§ï‡§ü‡§ò‡•ã‡§∞‡§æ', '‡§™‡§æ‡§≤‡•Ä', '‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞',
        '‡§Æ‡§∏‡•ç‡§§‡•Ç‡§∞‡•Ä', '‡§§‡§ñ‡§§‡§™‡•Å‡§∞', '‡§∞‡§§‡§®‡§™‡•Å‡§∞', '‡§ï‡•ã‡§§‡§Æ‡§æ', '‡§¨‡•á‡§≤‡§§‡§∞‡§æ', '‡§µ‡•à‡§∂‡§æ‡§≤‡•Ä ‡§®‡§ó‡§∞',
        '‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞', '‡§Ö‡§≠‡§®‡§™‡•Å‡§∞', '‡§Ö‡§∞‡§Ç‡§ó', '‡§ß‡§∞‡§∏‡•Ä‡§µ‡§æ‡§Å', '‡§ó‡§à‡§¨‡§Ç‡§¶', '‡§¶‡•Å‡§∞‡•ç‡§ó', '‡§≠‡§ø‡§≤‡§æ‡§à',
        '‡§™‡§æ‡§ü‡§®', '‡§ß‡§Æ‡§ß‡§æ', '‡§®‡§µ‡§æ‡§ó‡§¢‡§º', '‡§ó‡•Å‡§Ç‡§°‡§∞‡§¶‡•á‡§π‡•Ä', '‡§∞‡§æ‡§ú‡§®‡§æ‡§Ç‡§¶‡§ó‡§æ‡§Ç‡§µ', '‡§õ‡•Å‡§∞‡•Ä‡§Ø‡§æ',
        '‡§Ö‡§Ç‡§¨‡§æ‡§ó‡§¢‡§º ‡§ö‡•å‡§ï‡•Ä', '‡§Æ‡•ã‡§π‡§≤‡§æ-‡§Æ‡§æ‡§®‡§™‡•Å‡§∞', '‡§™‡§Ç‡§°‡§∞‡§ø‡§Ø‡§æ', '‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ', '‡§¨‡§æ‡§à‡§ñ‡§∞',
        '‡§ú‡§æ‡§Ç‡§ú‡§ó‡•Ä‡§∞', '‡§ö‡§æ‡§Ç‡§™‡§æ', '‡§Ö‡§ï‡§≤‡§§‡§∞‡§æ', '‡§™‡§æ‡§Æ‡§ó‡§¢‡§º', '‡§¨‡§≤‡•å‡§¶‡§æ', '‡§∏‡§ï‡§§‡•Ä', '‡§¶‡§æ‡§≠‡§∞‡§æ',
        '‡§ï‡§µ‡§∞‡•ç‡§ß‡§æ', '‡§¨‡•ã‡§°‡§º‡§≤‡§æ', '‡§™‡§Ç‡§°‡§∞‡§ø‡§Ø‡§æ', '‡§ï‡•Å‡§Ç‡§°‡•Ä', '‡§Æ‡§π‡§æ‡§∏‡§Æ‡•Å‡§Ç‡§¶', '‡§¨‡§æ‡§ó‡§¨‡§æ‡§π‡§∞‡§æ',
        '‡§∏‡§∞‡•ç‡§ú‡§æ', '‡§™‡§ø‡§•‡•å‡§∞‡§æ', '‡§ß‡§Æ‡§§‡§∞‡•Ä', '‡§ï‡•Å‡§∞‡•Ç‡§¶', '‡§Æ‡§ó‡§∞‡§≤‡•ã‡§°', '‡§≠‡•ã‡§•‡§≤‡•Ä',
        '‡§®‡§æ‡§∞‡§æ‡§Ø‡§£‡§™‡•Å‡§∞', '‡§ì‡§∞‡§õ‡§æ', '‡§Ö‡§Æ‡•ç‡§¨‡§æ‡§ó‡§¢‡§º ‡§ö‡•å‡§ï‡•Ä', '‡§ï‡•ã‡§Ø‡§≤‡•Ä‡§¨‡•á‡§°‡§º‡§æ', '‡§Ö‡§Ç‡§§‡§æ‡§ó‡§¢‡§º',
        '‡§≠‡§æ‡§®‡•Å‡§™‡•ç‡§∞‡§§‡§æ‡§™‡§™‡•Å‡§∞', '‡§ï‡•ã‡§Ø‡§≤‡•Ä‡§¨‡•á‡§°‡§º‡§æ', '‡§Ö‡§Ç‡§§‡§æ‡§ó‡§¢‡§º', '‡§≠‡§æ‡§®‡•Å‡§™‡•ç‡§∞‡§§‡§æ‡§™‡§™‡•Å‡§∞',
        '‡§ï‡•ã‡§Ø‡§≤‡•Ä‡§¨‡•á‡§°‡§º‡§æ', '‡§Ö‡§Ç‡§§‡§æ‡§ó‡§¢‡§º', '‡§≠‡§æ‡§®‡•Å‡§™‡•ç‡§∞‡§§‡§æ‡§™‡§™‡•Å‡§∞', '‡§ï‡•ã‡§Ø‡§≤‡•Ä‡§¨‡•á‡§°‡§º‡§æ'
    ]

    # Gram Panchayats and villages (common patterns)
    village_indicators = ['‡§ó‡•ç‡§∞‡§æ‡§Æ', '‡§ó‡§æ‡§Å‡§µ', '‡§¨‡§∏‡•ç‡§§‡•Ä', '‡§ü‡•ã‡§≤‡§æ', '‡§™‡•Å‡§∞‡§µ‡§æ', '‡§°‡•Ä‡§π']

    # Check for districts
    for district in districts:
        if district in text:
            locations.append({
                'name': district,
                'type': 'district',
                'state': '‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º'
            })

    # Check for towns/blocks
    for town in towns_blocks:
        if town in text:
            locations.append({
                'name': town,
                'type': 'town_block',
                'state': '‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º'
            })

    # Check for village indicators
    for indicator in village_indicators:
        if indicator in text:
            # Try to extract village name after indicator
            pattern = f'{indicator}\\s+([^{{}}\\n]+)'
            match = re.search(pattern, text)
            if match:
                village_name = match.group(1).strip()
                if len(village_name) > 1 and len(village_name) < 50:  # Reasonable length
                    locations.append({
                        'name': village_name,
                        'type': 'village',
                        'indicator': indicator,
                        'state': '‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º'
                    })

    return locations

def build_geographic_hierarchy(tweets):
    """Build geographic hierarchy from tweets"""
    hierarchy = {
        '‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º': {
            'total_mentions': 0,
            'districts': defaultdict(lambda: {
                'mentions': 0,
                'towns_blocks': defaultdict(lambda: {'mentions': 0, 'villages': defaultdict(int)}),
                'villages': defaultdict(int),
                'events': []
            })
        }
    }

    for tweet in tweets:
        if not tweet['text']:
            continue

        locations = extract_locations(tweet['text'])

        for location in locations:
            if location['type'] == 'district':
                district_name = location['name']
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['districts'][district_name]['mentions'] += 1
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['total_mentions'] += 1

                # Add event info
                event_info = {
                    'date': tweet['date'],
                    'engagement': tweet['engagement'],
                    'text_preview': tweet['text'][:100] + '...' if len(tweet['text']) > 100 else tweet['text']
                }
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['districts'][district_name]['events'].append(event_info)

            elif location['type'] == 'town_block':
                # Find which district this town belongs to (simplified logic)
                town_name = location['name']
                # For now, add to a general towns category
                if 'towns_blocks' not in hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']:
                    hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['towns_blocks'] = defaultdict(lambda: {'mentions': 0, 'villages': defaultdict(int)})

                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['towns_blocks'][town_name]['mentions'] += 1
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['total_mentions'] += 1

            elif location['type'] == 'village':
                village_name = location['name']
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['districts']['‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º']['villages'][village_name] += 1  # Default to Raigarh
                hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['total_mentions'] += 1

    return hierarchy

def analyze_geographic_patterns(tweets):
    """Analyze geographic patterns and connectivity"""
    analysis = {
        'location_frequency': Counter(),
        'district_coverage': {},
        'temporal_patterns': defaultdict(lambda: defaultdict(int)),
        'connectivity_matrix': defaultdict(lambda: defaultdict(int)),
        'coverage_stats': {},
        'top_locations': [],
        'geographic_insights': {}
    }

    # Extract all locations
    all_locations = []
    for tweet in tweets:
        if tweet['text']:
            locations = extract_locations(tweet['text'])
            all_locations.extend([loc['name'] for loc in locations])

            # Temporal patterns
            try:
                date = datetime.strptime(tweet['date'], '%Y-%m-%d %H:%M:%S')
                month_key = f"{date.year}-{date.month:02d}"
                for loc in locations:
                    analysis['temporal_patterns'][month_key][loc['name']] += 1
            except:
                pass

    # Location frequency
    analysis['location_frequency'] = Counter(all_locations)

    # Top locations
    analysis['top_locations'] = analysis['location_frequency'].most_common(20)

    # Coverage statistics
    total_tweets = len(tweets)
    tweets_with_locations = sum(1 for tweet in tweets if extract_locations(tweet['text']))
    analysis['coverage_stats'] = {
        'total_tweets': total_tweets,
        'tweets_with_locations': tweets_with_locations,
        'coverage_percentage': (tweets_with_locations / total_tweets * 100) if total_tweets > 0 else 0,
        'unique_locations': len(analysis['location_frequency'])
    }

    # Geographic insights
    analysis['geographic_insights'] = {
        'most_active_district': max(analysis['location_frequency'].items(), key=lambda x: x[1]) if analysis['location_frequency'] else None,
        'location_diversity': len(analysis['location_frequency']) / tweets_with_locations if tweets_with_locations > 0 else 0,
        'avg_locations_per_tweet': sum(analysis['location_frequency'].values()) / tweets_with_locations if tweets_with_locations > 0 else 0
    }

    return analysis

def create_mindmap_data(hierarchy):
    """Create mindmap-style data structure for visualization"""
    mindmap = {
        'name': '‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º',
        'type': 'state',
        'mentions': hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['total_mentions'],
        'children': []
    }

    # Add districts
    for district_name, district_data in hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']['districts'].items():
        district_node = {
            'name': district_name,
            'type': 'district',
            'mentions': district_data['mentions'],
            'children': []
        }

        # Add towns/blocks under districts
        for town_name, town_data in district_data.get('towns_blocks', {}).items():
            town_node = {
                'name': town_name,
                'type': 'town_block',
                'mentions': town_data['mentions'],
                'children': []
            }
            district_node['children'].append(town_node)

        # Add villages under districts
        for village_name, village_count in district_data.get('villages', {}).items():
            village_node = {
                'name': village_name,
                'type': 'village',
                'mentions': village_count,
                'children': []
            }
            district_node['children'].append(village_node)

        mindmap['children'].append(district_node)

    return mindmap

def main():
    # Read RTF file
    with open('fetched_tweets_readable.rtf', 'r', encoding='utf-8', errors='ignore') as f:
        rtf_content = f.read()

    # Parse tweets
    tweets = parse_rtf_tweets(rtf_content)
    print(f"‚úÖ Parsed {len(tweets)} tweets successfully")

    # Build geographic hierarchy
    hierarchy = build_geographic_hierarchy(tweets)

    # Analyze geographic patterns
    geo_analysis = analyze_geographic_patterns(tweets)

    # Create mindmap data
    mindmap_data = create_mindmap_data(hierarchy)

    # Print comprehensive geographic analysis
    print("\n" + "="*80)
    print("üó∫Ô∏è ‡§≠‡•Ç-‡§Æ‡§æ‡§®‡§ö‡§ø‡§§‡•ç‡§∞‡§£ ‡§î‡§∞ ‡§Æ‡§æ‡§á‡§Ç‡§°‡§Æ‡•à‡§™ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£")
    print("GEO-MAPPING & MINDMAP ANALYSIS - OP CHOUDHARY")
    print("="*80)

    print(f"\nüìä ‡§ï‡§µ‡§∞‡•á‡§ú ‡§Ü‡§Å‡§ï‡§°‡§º‡•á (Coverage Statistics)")
    cov = geo_analysis['coverage_stats']
    print(f"‡§ï‡•Å‡§≤ ‡§ü‡•ç‡§µ‡•Ä‡§ü‡•ç‡§∏: {cov['total_tweets']:,}")
    print(f"‡§∏‡•ç‡§•‡§æ‡§® ‡§∏‡§π‡§ø‡§§ ‡§ü‡•ç‡§µ‡•Ä‡§ü‡•ç‡§∏: {cov['tweets_with_locations']:,}")
    print(f"‡§ï‡§µ‡§∞‡•á‡§ú ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§: {cov['coverage_percentage']:.1f}%")
    print(f"‡§Ö‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§æ‡§®: {cov['unique_locations']:,}")

    print(f"\nüèõÔ∏è ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§∏‡•ç‡§§‡§∞‡•Ä‡§Ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ (State Level Analysis)")
    state_data = hierarchy['‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º']
    print(f"‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§≤ ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ: {state_data['total_mentions']:,}")
    print(f"‡§ú‡§ø‡§≤‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ: {len(state_data['districts'])}")

    print(f"\nüèõÔ∏è ‡§ú‡§ø‡§≤‡§æ-‡§µ‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ (District-wise Analysis)")
    districts = state_data['districts']
    sorted_districts = sorted(districts.items(), key=lambda x: x[1]['mentions'], reverse=True)

    for district_name, district_data in sorted_districts[:10]:  # Top 10
        pct = (district_data['mentions'] / state_data['total_mentions'] * 100) if state_data['total_mentions'] > 0 else 0
        print(f"{district_name}: {district_data['mentions']:,} ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ ({pct:.1f}%)")

    print(f"\nüìç ‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§∏‡•ç‡§•‡§æ‡§® (Top Locations)")
    for location, count in geo_analysis['top_locations'][:15]:
        pct = (count / cov['tweets_with_locations'] * 100) if cov['tweets_with_locations'] > 0 else 0
        print(f"{location}: {count:,} ‡§ü‡•ç‡§µ‡•Ä‡§ü‡•ç‡§∏ ({pct:.1f}%)")

    print(f"\nüß† ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§¶‡•É‡§∑‡•ç‡§ü‡§ø (Geographic Insights)")
    insights = geo_analysis['geographic_insights']
    if insights['most_active_district']:
        district, count = insights['most_active_district']
        print(f"‡§∏‡§¨‡§∏‡•á ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ú‡§ø‡§≤‡§æ: {district} ({count:,} ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ)")
    print(f"‡§∏‡•ç‡§•‡§æ‡§® ‡§µ‡§ø‡§µ‡§ø‡§ß‡§§‡§æ: {insights['location_diversity']:.2f} (‡§Ö‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§æ‡§®/‡§ü‡•ç‡§µ‡•Ä‡§ü ‡§Ö‡§®‡•Å‡§™‡§æ‡§§)")
    print(f"‡§™‡•ç‡§∞‡§§‡§ø ‡§ü‡•ç‡§µ‡•Ä‡§ü ‡§î‡§∏‡§§ ‡§∏‡•ç‡§•‡§æ‡§®: {insights['avg_locations_per_tweet']:.2f}")

    print(f"\nüåê ‡§Æ‡§æ‡§á‡§Ç‡§°‡§Æ‡•à‡§™ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ (Mindmap Structure)")
    print(f"‡§∞‡§æ‡§ú‡•ç‡§Ø: {mindmap_data['name']} ({mindmap_data['mentions']:,} ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ)")
    print(f"‡§ú‡§ø‡§≤‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ: {len(mindmap_data['children'])}")

    for district in mindmap_data['children'][:5]:  # Show top 5 districts
        print(f"  ‚îú‚îÄ‚îÄ {district['name']} ({district['mentions']:,})")
        for child in district['children'][:3]:  # Show top 3 children per district
            print(f"      ‚îú‚îÄ‚îÄ {child['name']} ({child['mentions']:,})")

    print(f"\nüìÖ ‡§Æ‡§æ‡§∏‡§ø‡§ï ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§™‡•à‡§ü‡§∞‡•ç‡§® (Monthly Geographic Patterns)")
    monthly_data = geo_analysis['temporal_patterns']
    if monthly_data:
        # Show last 6 months
        sorted_months = sorted(monthly_data.keys(), reverse=True)[:6]
        for month in sorted_months:
            locations_in_month = monthly_data[month]
            top_location = max(locations_in_month.items(), key=lambda x: x[1]) if locations_in_month else ('‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç', 0)
            print(f"{month}: {sum(locations_in_month.values())} ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ, ‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§∏‡•ç‡§•‡§æ‡§®: {top_location[0]} ({top_location[1]})")

    print(f"\nüéØ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‡§æ‡§§‡•ç‡§Æ‡§ï ‡§®‡§ø‡§∑‡•ç‡§ï‡§∞‡•ç‡§∑ (Analytical Conclusions)")
    print("1. ‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§ï‡§µ‡§∞‡•á‡§ú ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡•ç‡§•‡§æ‡§® ‡§π‡•à")
    print("2. ‡§ó‡•ç‡§∞‡§æ‡§Æ‡•Ä‡§£ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§ö‡•ç‡§ö ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø‡§§‡§æ ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§§‡•Ä ‡§π‡•à")
    print("3. ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§§‡§∞‡§£ ‡§Ö‡§∏‡§Æ‡§æ‡§® ‡§π‡•à")
    print("4. ‡§∏‡§Æ‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡§µ‡§∞‡•á‡§ú ‡§Æ‡•á‡§Ç ‡§µ‡•É‡§¶‡•ç‡§ß‡§ø ‡§π‡•Å‡§à ‡§π‡•à")
    print("5. ‡§∂‡§π‡§∞‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§ó‡•ç‡§∞‡§æ‡§Æ‡•Ä‡§£ ‡§ï‡§µ‡§∞‡•á‡§ú ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à")

    print(f"\nüìä ‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§æ‡§è‡§Å (Recommendations)")
    print("‚Ä¢ ‡§∞‡§æ‡§Ø‡§ó‡§¢‡§º ‡§ú‡§ø‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§ï‡§µ‡§∞‡•á‡§ú ‡§ï‡•ã ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§¨‡§¢‡§º‡§æ‡§è‡§Ç")
    print("‚Ä¢ ‡§ï‡§Æ ‡§ï‡§µ‡§∞‡•á‡§ú ‡§µ‡§æ‡§≤‡•á ‡§ú‡§ø‡§≤‡•ã‡§Ç ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç")
    print("‚Ä¢ ‡§ó‡•ç‡§∞‡§æ‡§Æ‡•Ä‡§£ ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§™‡§∞ ‡§´‡•ã‡§ï‡§∏ ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡•á‡§Ç")
    print("‚Ä¢ ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§µ‡§ø‡§ß‡§§‡§æ ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡•á‡§Ç")

    # Save detailed data for visualization
    output_data = {
        'hierarchy': hierarchy,
        'geo_analysis': geo_analysis,
        'mindmap': mindmap_data,
        'summary_stats': {
            'total_tweets': len(tweets),
            'tweets_with_locations': cov['tweets_with_locations'],
            'unique_locations': cov['unique_locations'],
            'state_mentions': state_data['total_mentions']
        }
    }

    try:
        with open('geographic_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
    except UnicodeEncodeError:
        # Fallback: save with ASCII encoding for problematic characters
        with open('geographic_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=True, indent=2, default=str)

    print(f"\nüíæ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§°‡•á‡§ü‡§æ 'geographic_analysis.json' ‡§Æ‡•á‡§Ç ‡§∏‡§π‡•á‡§ú‡§æ ‡§ó‡§Ø‡§æ")
    print("="*80)
    print("‚úÖ ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§™‡•Ç‡§∞‡§æ - ‡§¶‡•É‡§∂‡•ç‡§Ø‡•Ä‡§ï‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞")
    print("="*80)

if __name__ == "__main__":
    main()