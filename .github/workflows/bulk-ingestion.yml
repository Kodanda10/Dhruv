name: Manual Bulk Tweet Ingestion

on:
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of tweets to process per run'
        required: true
        default: '250'
        type: string
      start_offset:
        description: 'Starting offset for tweet processing'
        required: true
        default: '0'
        type: string
      total_tweets:
        description: 'Total number of tweets to process'
        required: true
        default: '2600'
        type: string
      dry_run:
        description: 'Run in dry-run mode (no database writes)'
        required: false
        default: false
        type: boolean
      partial_tolerance:
        description: 'Continue processing even if individual tweets fail'
        required: false
        default: true
        type: boolean
      run_reparse:
        description: 'Run reparse for failed tweets after main processing'
        required: false
        default: false
        type: boolean

jobs:
  bulk-parse-and-ingest:
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'

    - name: Install dependencies
      run: npm install

    - name: Install backend dependencies
      run: cd backend && npm install

    - name: Run Bulk Parsing and Ingestion
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        X_CLIENT_ID: ${{ secrets.X_CLIENT_ID }}
        X_CLIENT_SECRET: ${{ secrets.X_CLIENT_SECRET }}
        X_ACCESS_TOKEN: ${{ secrets.X_ACCESS_TOKEN }}
        X_ACCESS_TOKEN_SECRET: ${{ secrets.X_ACCESS_TOKEN_SECRET }}
        MAPMYINDIA_CLIENT_ID: ${{ secrets.MAPMYINDIA_CLIENT_ID }}
        MAPMYINDIA_CLIENT_SECRET: ${{ secrets.MAPMYINDIA_CLIENT_SECRET }}
        NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN: ${{ secrets.NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN }}
        REDIS_URL: ${{ secrets.REDIS_URL }}
        UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
        UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        API_BASE: 'http://127.0.0.1:3001'
        DRY_RUN: ${{ github.event.inputs.dry_run }}
      run: |
        # Start backend stub server in background
        cd backend && node app.js &
        SERVER_PID=$!
        echo "Backend server started with PID $SERVER_PID"

        # Wait for server to be ready
        echo "Waiting for server to start..."
        for i in {1..30}; do
          echo "Health check attempt $i/30..."
          if curl -s --fail http://127.0.0.1:3001/api/health > health_response.json 2>&1; then
            echo "âœ… Server health check passed!"
            echo "Health response:"
            cat health_response.json
            break
          else
            echo "âŒ Health check failed (attempt $i/30)"
            echo "Response:"
            cat health_response.json 2>/dev/null || echo "No response received"
            if [ $i -eq 30 ]; then
              echo "âŒ Server failed to become healthy after 30 attempts"
              exit 1
            fi
            sleep 2
          fi
        done

        # Loop through batches
        current_offset="${{ github.event.inputs.start_offset }}"
        batch_size="${{ github.event.inputs.batch_size }}"
        total_tweets="${{ github.event.inputs.total_tweets }}"
        partial_tolerance="${{ github.event.inputs.partial_tolerance || 'true' }}"

        while [ $current_offset -lt $total_tweets ]; do
          echo "=================================================="
          echo "Processing batch starting at offset: $current_offset"
          echo "=================================================="

          if [ "${DRY_RUN}" = "true" ]; then
            echo "Running in DRY RUN mode..."
            API_BASE=http://127.0.0.1:3001 node scripts/parse_tweets.js --batch=$batch_size --start=$current_offset --dry
          else
            echo "Running production parsing pipeline..."
            echo "Features enabled:"
            echo "  âœ… 3-layer consensus (Gemini + Ollama + FAISS)"
            echo "  âœ… Partial parse tolerance (continues on failures)"
            echo "  âœ… JSON verification and backup integrity"
            echo "  âœ… Automatic retry logic (3 attempts per tweet)"
            echo "  âœ… Reparse capability for failed tweets"
            API_BASE=http://127.0.0.1:3001 node scripts/parse_tweets.js --batch=$batch_size --start=$current_offset
          fi

          # Check exit code - only fail if critical error (not individual tweet failures)
          script_exit_code=$?
          if [ $script_exit_code -eq 2 ]; then
            echo "âŒ CRITICAL ERROR: Stopping workflow due to system failure."
            exit 1
          elif [ $script_exit_code -ne 0 ]; then
            echo "âš ï¸ Batch completed with some failures (partial tolerance enabled)"
            echo "Continuing with next batch..."
          fi

          current_offset=$((current_offset + batch_size))
          echo "Batch complete. Next offset: $current_offset"
          echo "Pausing for 15 seconds before next batch to respect rate limits..."
          sleep 15
        done

        echo "âœ… All batches processed successfully."
        
        # Optional: Run reparse for failed tweets
        if [ "${{ github.event.inputs.run_reparse || 'false' }}" = "true" ]; then
          echo "ðŸ”„ Running reparse for failed tweets..."
          API_BASE=http://127.0.0.1:3001 node scripts/parse_tweets.js --reparse
        fi

        # Cleanup
        kill $SERVER_PID 2>/dev/null || true
        echo "Server stopped."

    - name: Upload All Parsing Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: bulk-parsing-reports
        path: .workflow/reports/
        retention-days: 30

    - name: Upload All Backup Data
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: bulk-parsed-backups
        path: data/backups/
        retention-days: 7
