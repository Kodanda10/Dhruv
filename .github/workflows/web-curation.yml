name: web-curation

on:
  workflow_dispatch:
  schedule:
    # Nightly at 02:00 UTC (adjust as needed)
    - cron: '0 2 * * *'

permissions:
  contents: write
  pull-requests: write

concurrency:
  group: web-curation
  cancel-in-progress: false

jobs:
  curate-and-propose:
    name: Curate web mappings
    runs-on: ubuntu-latest
    # always run (flag gate removed)
    env:
      # Feature flags (overridable via repo Variables)
      ENABLE_AUTONOMOUS_WEB_CURATION: ${{ vars.ENABLE_AUTONOMOUS_WEB_CURATION || 'true' }}
      ENABLE_SEARCH_AUTOMATION: ${{ vars.ENABLE_SEARCH_AUTOMATION || 'false' }}
      ENABLE_WEB_SCRAPING: ${{ vars.ENABLE_WEB_SCRAPING || 'false' }}
      ENABLE_WEB_CACHE: 'true'

      # Optional search credentials (set in repo Secrets)
      GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      BING_SEARCH_KEY: ${{ secrets.BING_SEARCH_KEY }}

      # Ensure module imports resolve
      PYTHONPATH: api/src
      # Strict Google CSE quotas and rate limit (never exceed free 100/day)
      CSE_DAILY_BUDGET: '100'
      CSE_RATE_LIMIT_S: '1.5'

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run autonomous web curation (batch)
        id: curate
        run: |
          set -euo pipefail
          echo "ENABLE_AUTONOMOUS_WEB_CURATION=${ENABLE_AUTONOMOUS_WEB_CURATION}"
          echo "ENABLE_SEARCH_AUTOMATION=${ENABLE_SEARCH_AUTOMATION}"
          echo "ENABLE_WEB_SCRAPING=${ENABLE_WEB_SCRAPING}"
          # Limit per-run attempts to be polite to providers; adjust as needed
          python api/src/sota/dataset_builders/tools/web_curation.py --limit 200 --force --cse-daily-budget 100 --cse-rate-limit-s 1.5
          echo "curation_summary<<JSON" >> $GITHUB_OUTPUT
          python - <<'PY'
          import json, os
          p="data/name_mappings/autocurated/geography_name_map.ndjson"
          exists=os.path.exists(p)
          print(json.dumps({"ndjson_written": exists, "path": p}))
          PY
          echo "JSON" >> $GITHUB_OUTPUT

      - name: Show proposed changes (if any)
        run: |
          git status
          echo "---- Diff (name_mappings) ----"
          git diff --stat -- data/name_mappings || true

      - name: Create PR with curated mappings
        id: cpr
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: bot/web-curation
          base: ${{ github.event.repository.default_branch }}
          add-paths: |
            data/name_mappings/**
            !data/.web_cache/**
          commit-message: 'chore(web-curation): auto-curated mappings from authoritative portals'
          title: 'chore(web-curation): auto-curated mappings'
          body: |
            This automated PR proposes curated Hindi/Nukta-Hindi mappings verified from allowlisted authoritative sources (gov/nic/census/egramswaraj).
            - Flags: ENABLE_AUTONOMOUS_WEB_CURATION=${{ env.ENABLE_AUTONOMOUS_WEB_CURATION }}, SEARCH=${{ env.ENABLE_SEARCH_AUTOMATION }}, SCRAPE=${{ env.ENABLE_WEB_SCRAPING }}
            - NDJSON: data/name_mappings/autocurated/geography_name_map.ndjson
            - JSON map auto-merged: data/name_mappings/geography_name_map.json
            - Cache: data/.web_cache (not committed)
          labels: |
            automation
            web-curation
          author: 'web-curator-bot <actions@github.com>'
          committer: 'web-curator-bot <actions@github.com>'

      - name: PR result
        if: steps.cpr.outputs.pull-request-number
        run: |
          echo "Created PR #${{ steps.cpr.outputs.pull-request-number }} at ${{ steps.cpr.outputs.pull-request-url }}"
