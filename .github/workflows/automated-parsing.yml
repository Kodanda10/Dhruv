name: Automated Tweet Parsing & Ingestion

on:
  schedule:
    # Run daily at 2 AM IST (8:30 PM UTC)
    - cron: '30 20 * * *'
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of tweets to process per batch'
        required: false
        default: '50'
        type: string
      start_offset:
        description: 'Starting offset for tweet processing'
        required: false
        default: '0'
        type: string
      dry_run:
        description: 'Run in dry-run mode (no database writes)'
        required: false
        default: false
        type: boolean

jobs:
  parse-and-ingest:
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install psycopg2-binary

    - name: Wait for services
      run: |
        echo "Waiting for external services to be ready..."
        sleep 10

    - name: Run automated parsing pipeline
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        X_CLIENT_ID: ${{ secrets.X_CLIENT_ID }}
        X_CLIENT_SECRET: ${{ secrets.X_CLIENT_SECRET }}
        X_ACCESS_TOKEN: ${{ secrets.X_ACCESS_TOKEN }}
        X_ACCESS_TOKEN_SECRET: ${{ secrets.X_ACCESS_TOKEN_SECRET }}
        MAPMYINDIA_CLIENT_ID: ${{ secrets.MAPMYINDIA_CLIENT_ID }}
        MAPMYINDIA_CLIENT_SECRET: ${{ secrets.MAPMYINDIA_CLIENT_SECRET }}
        NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN: ${{ secrets.NEXT_PUBLIC_MAPBOX_ACCESS_TOKEN }}
        REDIS_URL: ${{ secrets.REDIS_URL }}
        UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
        UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        API_BASE: ${{ secrets.API_BASE }}
        BATCH_SIZE: ${{ github.event.inputs.batch_size || '50' }}
        START_OFFSET: ${{ github.event.inputs.start_offset || '0' }}
        DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
      run: |
        # Start Next.js server in background
        npm run build
        timeout 300 npm run start &
        SERVER_PID=$!

        # Wait for server to be ready
        echo "Waiting for server to start..."
        for i in {1..30}; do
          if curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
            echo "Server is ready!"
            break
          fi
          sleep 2
        done

        # Run parsing pipeline
        if [ "$DRY_RUN" = "true" ]; then
          echo "Running in DRY RUN mode..."
          API_BASE=http://localhost:3000 node scripts/parse_tweets.js --batch=$BATCH_SIZE --start=$START_OFFSET --dry
        else
          echo "Running production parsing pipeline..."
          API_BASE=http://localhost:3000 node scripts/parse_tweets.js --batch=$BATCH_SIZE --start=$START_OFFSET
        fi

        # Cleanup
        kill $SERVER_PID 2>/dev/null || true

    - name: Upload parsing reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: parsing-reports
        path: .workflow/reports/
        retention-days: 30

    - name: Upload backup data
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: parsed-backups
        path: data/backups/
        retention-days: 7

    - name: Generate summary report
      if: always()
      run: |
        echo "# Automated Parsing Summary" > summary.md
        echo "" >> summary.md
        echo "**Run Date:** $(date)" >> summary.md
        echo "**Batch Size:** ${{ github.event.inputs.batch_size || '50' }}" >> summary.md
        echo "**Start Offset:** ${{ github.event.inputs.start_offset || '0' }}" >> summary.md
        echo "**Dry Run:** ${{ github.event.inputs.dry_run || 'false' }}" >> summary.md
        echo "" >> summary.md

        if [ -f ".workflow/reports/report_${{ github.event.inputs.start_offset || '0' }}_${{ github.event.inputs.batch_size || '50' }}.json" ]; then
          echo "**Results:**" >> summary.md
          cat ".workflow/reports/report_${{ github.event.inputs.start_offset || '0' }}_${{ github.event.inputs.batch_size || '50' }}.json" | jq -r '
            "âœ… Successfully processed: \(.ok)",
            "â³ Needs review: \(.needs_review)",
            "âŒ Errors: \(.errors)",
            "ðŸ”„ Retries: \(.retried)",
            "",
            "**Processed Tweet IDs:**",
            (.ids[] | "- \(.id): \(if .ok then "âœ… Auto-approved" elif .error then "âŒ Error: \(.error)" else "â³ Needs review" end)")
          ' >> summary.md 2>/dev/null || echo "Could not parse report JSON" >> summary.md
        else
          echo "**Results:** No report file found" >> summary.md
        fi

    - name: Comment on issue/PR
      if: github.event_name == 'workflow_dispatch'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('summary.md', 'utf8');

          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

    - name: Notify on failures
      if: failure()
      run: |
        echo "Parsing pipeline failed. Check the logs and artifacts for details."
        # Add notification logic here (Slack, Discord, etc.)